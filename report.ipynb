{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/ES/dev.out ../Data/ES/dev.p1.out` <br>\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/RU/dev.out ../Data/RU/dev.p1.out` <br>\n",
    "***\n",
    "#### Results for ES: \n",
    "Entity in gold data: 229 <br>\n",
    "Entity in prediction: 1241 <br>\n",
    "\n",
    "Correct Entity : 180 <br>\n",
    "Entity  precision: 0.1450 <br>\n",
    "Entity  recall: 0.7860 <br>\n",
    "Entity  F: 0.2449 <br>\n",
    "\n",
    "Correct Sentiment : 112 <br>\n",
    "Sentiment  precision: 0.0902 <br>\n",
    "Sentiment  recall: 0.4891 <br>\n",
    "Sentiment  F: 0.1524 <br>\n",
    "***\n",
    "#### Results for RU: \n",
    "Entity in gold data: 389 <br>\n",
    "Entity in prediction: 1521 <br>\n",
    "\n",
    "Correct Entity : 289 <br>\n",
    "Entity  precision: 0.1900 <br>\n",
    "Entity  recall: 0.7429 <br>\n",
    "Entity  F: 0.3026 <br>\n",
    "\n",
    "Correct Sentiment : 161 <br>\n",
    "Sentiment  precision: 0.1059 <br>\n",
    "Sentiment  recall: 0.4139 <br>\n",
    "Sentiment  F: 0.1686 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/ES/dev.out ../Data/ES/dev.p2.out` <br>\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/RU/dev.out ../Data/RU/dev.p2.out` <br>\n",
    "***\n",
    "#### Results for ES: \n",
    "Entity in gold data: 229<br>\n",
    "Entity in prediction: 542<br>\n",
    "\n",
    "Correct Entity : 134<br>\n",
    "Entity  precision: 0.2472<br>\n",
    "Entity  recall: 0.5852<br>\n",
    "Entity  F: 0.3476<br>\n",
    "\n",
    "Correct Sentiment : 97<br>\n",
    "Sentiment  precision: 0.1790<br>\n",
    "Sentiment  recall: 0.4236<br>\n",
    "Sentiment  F: 0.2516<br>\n",
    "***\n",
    "#### Results for RU: \n",
    "Entity in gold data: 389<br>\n",
    "Entity in prediction: 530<br>\n",
    "\n",
    "Correct Entity : 191<br>\n",
    "Entity  precision: 0.3604<br>\n",
    "Entity  recall: 0.4910<br>\n",
    "Entity  F: 0.4157<br>\n",
    "\n",
    "Correct Sentiment : 130<br>\n",
    "Sentiment  precision: 0.2453<br>\n",
    "Sentiment  recall: 0.3342<br>\n",
    "Sentiment  F: 0.2829<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the k-th best sequences, the Viterbi Algorithm can be modified. In our project, we implemented the Viterbi Algorithm from Part 2. In Part 2, we only stored the best value and tag in π(n, v) (which is denoted as `policy_dict` in the code). In Part 3, we have changed it to keep track of the k best values and tags in π(n, v).\n",
    "\n",
    "1. Obtain parameters for transition and emission paramters. <br>\n",
    "2. Initialize probabilities for each state for the first word. <br>\n",
    "3. (Forward recursion): For the words, calculate the max probabilities of the next states given the previous state's probabilities. => maxv{π(j, v) × bu(xj+1) × av,u}. We store k best tags and its probability values in an array `kth_ls`, which will be added to `policy_dict`<br>\n",
    "4. For the last word to STOP, calculate the value as such =>  maxv{π(n, v) × av,STOP}<br>\n",
    "5. (Backtracking): Since each state has its k highest probabilities stored in policy_dict, we can backtrack the stored k best probability and values from the last word to find the k best paths. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/ES/dev.out ../Data/ES/dev.p3.2nd.out` <br>\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/ES/dev.out ../Data/ES/dev.p3.8th.out` <br>\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/RU/dev.out ../Data/RU/dev.p3.2nd.out` <br>\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/RU/dev.out ../Data/RU/dev.p3.8th.out` <br>\n",
    "***\n",
    "#### Results for ES -- 2nd best sequence: \n",
    "Entity in gold data: 229<br>\n",
    "Entity in prediction: 573<br>\n",
    "\n",
    "Correct Entity : 119<br>\n",
    "Entity  precision: 0.2077<br>\n",
    "Entity  recall: 0.5197<br>\n",
    "Entity  F: 0.2968<br>\n",
    "\n",
    "Correct Sentiment : 66<br>\n",
    "Sentiment  precision: 0.1152<br>\n",
    "Sentiment  recall: 0.2882<br>\n",
    "Sentiment  F: 0.1646<br>\n",
    "***\n",
    "#### Results for ES -- 8th best sequence: \n",
    "Entity in gold data: 229<br>\n",
    "Entity in prediction: 867<br>\n",
    "\n",
    "Correct Entity : 126<br>\n",
    "Entity  precision: 0.1453<br>\n",
    "Entity  recall: 0.5502<br>\n",
    "Entity  F: 0.2299<br>\n",
    "\n",
    "Correct Sentiment : 78<br>\n",
    "Sentiment  precision: 0.0900<br>\n",
    "Sentiment  recall: 0.3406<br>\n",
    "Sentiment  F: 0.1423<br>\n",
    "***\n",
    "#### Results for RU -- 2nd best sequence: \n",
    "Entity in gold data: 389<br>\n",
    "Entity in prediction: 1068<br>\n",
    "\n",
    "Correct Entity : 206<br>\n",
    "Entity  precision: 0.1929<br>\n",
    "Entity  recall: 0.5296<br>\n",
    "Entity  F: 0.2828<br>\n",
    "\n",
    "Correct Sentiment : 127<br>\n",
    "Sentiment  precision: 0.1189<br>\n",
    "Sentiment  recall: 0.3265<br>\n",
    "Sentiment  F: 0.1743<br>\n",
    "***\n",
    "#### Results for RU -- 8th best sequence: \n",
    "Entity in gold data: 389<br>\n",
    "Entity in prediction: 1852<br>\n",
    "\n",
    "Correct Entity : 213<br>\n",
    "Entity  precision: 0.1150<br>\n",
    "Entity  recall: 0.5476<br>\n",
    "Entity  F: 0.1901<br>\n",
    "\n",
    "Correct Sentiment : 125<br>\n",
    "Sentiment  precision: 0.0675<br>\n",
    "Sentiment  recall: 0.3213<br>\n",
    "Sentiment  F: 0.1116<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Design Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Approach - Viterbi Algorithm with State Transition Positional Bias\n",
    "Using mean and standard deviation of position of state transition as bias in standard Viterbi Algorithm.<br>\n",
    "***\n",
    "#### Hyperparameters\n",
    "Mean: Average position of state transitions relative to length of state sequence [0,1]<br>\n",
    "Spread: Standard Deviation of position of state transitions relative to length of state sequence [0,1]<br>\n",
    "Hyperparameter model values selected as local maxima for total F-score.<br>\n",
    "***\n",
    "#### Evaluation\n",
    "\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/ES/dev.out ../Data/ES/dev.p4b.out` <br>\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/RU/dev.out ../Data/RU/dev.p4b.out` <br>\n",
    "***\n",
    "#### Results for ES - 1st Approach: \n",
    "Entity in gold data: 229<br>\n",
    "Entity in prediction: 413<br>\n",
    "\n",
    "Correct Entity : 135<br>\n",
    "Entity  precision: 0.3269<br>\n",
    "Entity  recall: 0.5895<br>\n",
    "Entity  F: 0.4206<br>\n",
    "\n",
    "Correct Sentiment : 100<br>\n",
    "Sentiment  precision: 0.2421<br>\n",
    "Sentiment  recall: 0.4367<br>\n",
    "Sentiment  F: 0.3115<br>\n",
    "\n",
    "Significant improvement in results.<br>\n",
    "***\n",
    "#### Results for RU - 1st Approach: \n",
    "Entity in gold data: 389<br>\n",
    "Entity in prediction: 528<br>\n",
    "\n",
    "Correct Entity : 189<br>\n",
    "Entity  precision: 0.3580<br>\n",
    "Entity  recall: 0.4859<br>\n",
    "Entity  F: 0.4122<br>\n",
    "\n",
    "Correct Sentiment : 129<br>\n",
    "Sentiment  precision: 0.2443<br>\n",
    "Sentiment  recall: 0.3316<br>\n",
    "Sentiment  F: 0.2814<br>\n",
    "\n",
    "No improvement in results.<br>\n",
    "The approach appears to be ineffective on languages with a highly flexible sentence structure. (in this case, Russian)<br>\n",
    "***\n",
    "### 2nd Approach - Naive Bayes\n",
    "<b>Finding maxP(word|label) to predict word sequence</b><br>\n",
    "#### Laplace Smoothing \n",
    "In order to solve the a feature (word) giving zero probability (exists in test but not training set), we use Laplace Smoothing. \n",
    "\n",
    "Example: P(x’/positive)= (number of reviews with x’ and target_outcome=positive + α) / (N+ α*k) \n",
    "\n",
    "In this function, we let α=1. This ensures that the posterior probability comes out to 1/N+k rather than zero.\n",
    "\n",
    "https://www.cs.rhodes.edu/~kirlinp/courses/ai/f18/projects/proj3/naive-bayes-log-probs.pdf\n",
    "***\n",
    "#### Evaluation\n",
    "\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/ES/dev.out ../Data/ES/dev.p4b.out` <br>\n",
    "from `/EvalScript/`, run `python evalResult.py ../Data/RU/dev.out ../Data/RU/dev.p4b.out` <br>\n",
    "***\n",
    "#### Results for ES - 2nd Approach: \n",
    "Entity in gold data: 229<br>\n",
    "Entity in prediction: 95<br>\n",
    "\n",
    "Correct Entity : 75<br>\n",
    "Entity  precision: 0.7895<br>\n",
    "Entity  recall: 0.3275<br>\n",
    "Entity  F: 0.4630<br>\n",
    "\n",
    "Correct Sentiment : 61<br>\n",
    "Sentiment  precision: 0.6421<br>\n",
    "Sentiment  recall: 0.2664<br>\n",
    "Sentiment  F: 0.3765<br>\n",
    "\n",
    "Greater improvement in results over 1st approach.<br>\n",
    "***\n",
    "#### Results for RU - 2nd Approach: \n",
    "Entity in gold data: 389<br>\n",
    "Entity in prediction: 89<br>\n",
    "\n",
    "Correct Entity : 77<br>\n",
    "Entity  precision: 0.8652<br>\n",
    "Entity  recall: 0.1979<br>\n",
    "Entity  F: 0.3222<br>\n",
    "\n",
    "Correct Sentiment : 54<br>\n",
    "Sentiment  precision: 0.6067<br>\n",
    "Sentiment  recall: 0.1388<br>\n",
    "Sentiment  F: 0.2259<br>\n",
    "\n",
    "Significant decline in results over 1st approach.<br>\n",
    "The approach appears to be even more sensitive than the 1st to language sentence structure flexibility.<br>\n",
    "This is likely due to the assumption of independant probabilities made in taking a Naive Bayes approach.<br>\n",
    "***\n",
    "### Chosen Approach: \"Viterbi Algorithm with State Transition Positional Bias\"\n",
    "This approach has higher F-score.\n",
    "\n",
    "Viterbi algorithm is better suited for capturing dependecies in word sequences. Even though Naive Bayes fairs better in ES, language is contextual. Viterbi leverages context infromation which improves the accuracy. This fairs better than Naive Bayes which treats each word independently.\n",
    "\n",
    "Compared to our design in part 3, we now consider positional bias. In language, certian entity types are more likely to appear at different parts of the sentence. By including positional bias, the viterbi algorithm gives more influence to the state at which it is most likely to appear in the sentence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
